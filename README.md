## ğŸ–¼ï¸ Screenshots

### ğŸ§­ Architecture Diagram
An overview of the deployed architecture on Kubernetes, including PostgreSQL, Keycloak, OAuth2-proxy, and the Dashboard.

![Architecture Diagram](screenshots/Architecture.png)



âœ… Kubernetes Cluster Setup (Single Node)
Created an EC2 t3.medium with 2 CPU and 20GB Ubuntu 22.04.

ğŸ”§ 1. System Preparation
Configured the system with required kernel modules, disabled swap, and applied sysctl values.

ğŸ³ 2. Install Container Runtime (containerd)
Installed containerd and enabled the systemd cgroup driver.

â˜¸ï¸ 3. Install Kubernetes Tools
Installed the following components from the Kubernetes repository:

kubeadm: Cluster initializer

kubelet: Node agent

kubectl: CLI for managing the cluster

ğŸš€ 4. Initialize the Cluster
Initialized the Kubernetes control plane using kubeadm with Calico-compatible pod CIDR (192.168.0.0/16). After initialization:

Configured kubectl access for the current user

ğŸŒ 5. Configured Network Add-on (Calico)
Deployed the Calico CNI plugin to enable pod networking.

ğŸŸ¢ 6. Enable Master Node Scheduling
Allowed the master node to schedule workloads (since this is a single-node setup).

âœ… 7. Verification
Ran the following commands to verify successful installation:

kubectl get nodes
kubectl get pods -A

---

### ğŸ“¦ Cluster Pods
Shows all running pods across namespaces, confirming the successful deployment of all components.

![Cluster Pods](screenshots/Clusterpods.png)




ğŸ˜ PostgreSQL Deployment
This step sets up a standalone PostgreSQL instance inside the Kubernetes cluster using a StatefulSet, PersistentVolumeClaim, and a ClusterIP service. It is securely configured with credentials, persistent storage, resource limits, health probes, and an automation script.

âš™ï¸ Key Components
ğŸ” Secret â€“ Stores database credentials (username, password, and database name)
ğŸ“¦ PVC â€“ Ensures PostgreSQL data is stored persistently
ğŸ§± StatefulSet â€“ Manages the PostgreSQL pod with stable identity and storage
ğŸŒ ClusterIP Service â€“ Internal-only service with stable DNS for PostgreSQL access

ğŸ’¡ Enhancements Added
âœ… Liveness Probe â€“ Automatically restarts the pod if PostgreSQL becomes unresponsive
âœ… Readiness Probe â€“ Ensures PostgreSQL is ready before exposing to other services
âœ… Resource Requests & Limits â€“ Controls CPU and memory usage for stability

âœ… Verification
The following commands were used to validate the PostgreSQL deployment:

kubectl get pods -l app=postgres
kubectl exec -it postgres-0 -- psql -U keycloak -d keycloakdb

---

## ğŸ›¢ï¸ PostgreSQL Deployment

#### âœ… PostgreSQL Pod Running
![PostgreSQL Pod](screenshots/postgre.png)

#### ğŸ’¾ Persistent Volume Claim
![PostgreSQL Storage](screenshots/postgre2.png)

---




ğŸ” Keycloak Deployment (Integrated with PostgreSQL)
Deployment of Keycloak in a single-node Kubernetes cluster, connected to the previously deployed PostgreSQL instance, and exposed securely using a NodePort-based HTTPS setup.

ğŸš€ 1. Keycloak Deployment
Keycloak was deployed using the official image quay.io/keycloak/keycloak:21.1.1 in development mode (start-dev). Environment variables were configured to connect with PostgreSQL via secrets.

The deployment includes:

Admin user and password setup

Database host, username, password, and name configuration using Kubernetes secrets

Container exposed on port 8080 internally

ğŸŒ 2. Service Configuration
A NodePort service was created to expose Keycloak on port 31319.

ğŸ“¦ 3. Applying Manifests
Manifests used:

kubectl apply -f keycloak-deployment.yaml
kubectl apply -f keycloak-hpa.yaml
kubectl apply -f keycloak-service.yaml
kubectl apply -f keycloak-ingress.yaml

âœ… 4. Verification
Checked that the Keycloak pod and services are running:
kubectl get pods -l app=keycloak
kubectl get svc keycloak

ğŸ“¡ 5. Host Access
Keycloak was accessed through the NodePort using a self-signed TLS certificate:

Self-signed certificate generated using openssl

TLS secret created in Kubernetes

Host entry added in /etc/hosts: <EC2_PUBLIC_IP> keycloak.local

ğŸŒ 6. Accessing Keycloak
Accessed via HTTPS using NodePort:

âœ… In Browser:
https://keycloak.local:31319

## ğŸ” Keycloak Admin Console

#### ğŸ”‘ Login Page
![Keycloak Login](screenshots/keycloak1.png)

#### âš™ï¸ Admin Console (Clients/Realm)
![Keycloak Admin](screenshots/keycloak2.png)

#### ğŸ‘¤ User Management
![Keycloak Users](screenshots/keycloak3.png)


ğŸ§° Dashboard Installation
The official Kubernetes Dashboard components were deployed into the kubernetes-dashboard namespace:

kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.8.0/aio/deploy/recommended.yaml

ğŸ‘¤ 2. Admin Access Configuration
To access the dashboard with admin privileges, a ServiceAccount and ClusterRoleBinding were created:
kubectl apply -f dashboard-adminuser.yaml

ğŸ”‘ 3. Generate Bearer Token
A token was generated to authenticate with the dashboard UI:
kubectl -n kubernetes-dashboard create token admin-user

ğŸ” 4. Secure Exposure via NodePort + Self-Signed TLS
The dashboard service was accessed securely using HTTPS via a NodePort:

A self-signed TLS certificate was generated using openssl

TLS secret created in kubernetes-dashboard namespace

Ingress configured with host: dashboard.local

ğŸŒ 5. Local DNS Resolution
The local machineâ€™s /etc/hosts was updated to route dashboard.local to the EC2 public IP:
<EC2_PUBLIC_IP> dashboard.local
This enabled browser access using the custom domain.

âœ… 6. Accessing the Dashboard
Dashboard UI was successfully accessed via:
https://dashboard.local:30595
Login was done using the bearer token generated in Step 3.


## ğŸ“Š Kubernetes Dashboard

#### ğŸ” Dashboard Login
![Dashboard Login](screenshots/dashboard1.png)

#### ğŸ“‹ Dashboard Overview
![Dashboard Overview](screenshots/dashboard2.png)

#### ğŸ—‚ï¸ Namespaces View
![Namespaces](screenshots/dashboard3.png)

#### ğŸ“¦ Pods View
![Pods](screenshots/dashboard4.png)

#### ğŸ“ˆ Metrics View
![Metrics](screenshots/dashboard5.png)





ğŸ›¡ï¸ OAuth2 Proxy Secured Whoami Application with Keycloak OIDC Integration
ğŸ”¹ What it is:
A simple Whoami app secured by OAuth2 Proxy using Keycloak as the OpenID Connect (OIDC) identity provider.

ğŸ” Architecture Overview
ğŸ–¥ï¸ Whoami App â€” Minimal HTTP service showing request info (for testing auth).

ğŸ” OAuth2 Proxy â€” Auth proxy validating users via Keycloak OIDC tokens.

ğŸ”‘ Keycloak â€” Central authentication provider issuing OAuth2/OIDC tokens.

â˜¸ï¸ Kubernetes Deployment â€” Both apps run as pods exposed externally.

ğŸšª Exposure & Access
ğŸ”Œ NodePort Service â€” OAuth2 Proxy is exposed via NodePort (30180) on the Kubernetes node.

ğŸŒ Hostnames configured in /etc/hosts like oauth2-proxy.local and whoami.local pointing to the cluster IP for convenience.

ğŸ”’ TLS â€” HTTPS enabled on OAuth2 Proxy using self-signed certificates.

âš™ï¸ How It Works
User accesses https:<EC2_PUBLIC_IP>//:30180)

OAuth2 Proxy redirects unauthenticated users to Keycloak login.

After login, Keycloak issues a token which OAuth2 Proxy validates.

Authenticated requests are forwarded to Whoami with user info headers.

Whoami responds confirming authenticated access.


## ğŸŒ Whoami OAuth2 Demo

A working demo of OAuth2 Proxy with the Whoami app for identity testing.

![Whoami App](screenshots/whoami.png)




